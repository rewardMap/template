meta:
  name: "Non-stationary-bandit"
  version: 0.0.1
  reduced_actions: null
graph:
  # Example task structure.
  # The agent starts in state 0
  0:
    # After taking action 0 (e.g. left) or 1 (e.g. right)
    # they get to the next state (1) with 80 % probability or (2) with 20 %
    0:
      next: [1, 2] # Indicate the state the agent is visiting next
      prob: 0.8 # This describes the transition probability.
    1:
      next: [2, 1]
      prob: 0.8
  1: [] # Arriving in state 1, there's a binary choice (3, 4) where action 0
            # takes the agent deterministically to state 3 and action 1 to state 4.
  2: []
rewards:
  1:
    class: BaseReward
    reward: [1, 0]
    p: [0.8, 0.2]

  2:
    class: PseudoRandomReward
    reward_list: [-1, -1, -1, -1, -1, -1, -1, -1, 0, 0]
