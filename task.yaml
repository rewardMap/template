meta:
  name: "Non-stationary-bandit"
  version: 0.0.1
  reduced_actions: null
graph:
  # Example task structure.
  # The agent starts in state 0
  0:
    # After taking action 0 (e.g. left) or 1 (e.g. right)
    # they get to the next state (1) with 80 % probability or (2) with 20 %
    0:
      next: [1, 2] # Indicate the state the agent is visiting next
      prob: 0.8 # This describes the transition probability.
    1:
      next: [2, 1]
      prob: 0.8
  1: [3, 4] # Arriving in state 1, there's a binary choice (3, 4) where action 0
            # takes the agent deterministically to state 3 and action 1 to state 4.
  2: [5, 6]
  3: []
  4: []
  5: []
  6: []
reward:
  3:
    class: BaseReward
    reward: 1
    p: 0.8

  4:
    class: PseudoRandomReward
    reward_list: [-1, -1, -1, -1, -1, -1, -1, -1, 0, 0]

  5:
    class: DriftingReward
    reward: [1, 0]
    borders: [0.25, 0.75]

  6:
    class: PseudoRandomReward
    reward_list: [1, 1, 0, 0, 0, 0, 0, 0, 0, 0]
